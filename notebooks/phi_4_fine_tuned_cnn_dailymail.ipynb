{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning Phi-4 for Text Summarization\n",
        "\n",
        "This notebook fine-tunes the Phi-4 model for Text Summarization using the `abisee/cnn_dailymail` dataset. It includes dataset downloading, preprocessing, fine-tuning with LoRA and Unsloth, and evaluation with Accuracy and training loss plotting.\n",
        "\n",
        "## Setup\n",
        "- **Environment**: Google Colab with T4 GPU (16GB VRAM).\n",
        "- **Libraries**: Unsloth for efficient fine-tuning, Hugging Face Transformers, Datasets, and Evaluate for metrics.\n",
        "- **Dataset**: `abisee/cnn_dailymail`.\n",
        "- **Output**: Fine-tuned model, ROUGE-L, and loss graph.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RJp8lneRw0y-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Iifn3dAnMFrK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Model and tokennizer\n",
        "Load the Phi-4 model with 4-bit quantization using Unsloth for memory efficiency."
      ],
      "metadata": {
        "id": "-H3YGrD7xSiG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6f37294263364bdd95e50916cec83ddc",
            "49659e262cf34db59bdf062e30b13d5c",
            "3c048397fe4e43c79b1c20ef5fbf4281",
            "350ae176c316473b98f846bb7f56e2ad",
            "4f4f50a93e74439f9bece3cc8953a529",
            "4fc276b800f0469f83dc767312d7252d",
            "fe51d860090c4dbb8952ddbd2bc0ca65",
            "45457026c9bc4dd1b1780c7561da8baa",
            "28bef971c9b14b9c8e9a1a5d4bd02c64",
            "6ba9efc9d55046ec91622665d701275a",
            "2505c01d00dd4cad81c8f78953de4a11"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "z1NLLxWsMFrM",
        "outputId": "5eceae72-eea5-4817-9e83-0c5bd2e28796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.5.9: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f37294263364bdd95e50916cec83ddc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-4\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configure Model with LoRA adapter\n",
        "\n",
        "Use LoRA for parameter-efficient fine-tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "DjM8drHWxXPn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-XJbOGVeMFrO",
        "outputId": "837fff30-322d-49ea-d9fe-3f189e583659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.5.9 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load and Preprocess Dataset\n",
        "\n",
        "We load the `abisee/cnn_dailymail` dataset and preprocess it to extract classification tasks.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_XqokBzyxaEG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsxaZYlMMFrP",
        "outputId": "e3a3da3e-1d1d-483c-bc3c-86d1e7e71a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted sample:\n",
            "<|im_start|>user<|im_sep|>Please summarize the following article:\n",
            "\n",
            "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported Â£20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest Â» . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.<|im_end|><|im_start|>assistant<|im_sep|>Harry Potter star Daniel Radcliffe gets Â£20M fortune as he turns 18 Monday .\n",
            "Young actor says he has no plans to fritter his cash away .\n",
            "Radcliffe's earnings from first five Potter films have been held in trust fund .<|im_end|>\n",
            "\n",
            "==================================================\n",
            "\n",
            "<|im_start|>user<|im_sep|>Please summarize the following article:\n",
            "\n",
            "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they're ready to appear in court. Most often, they face drug charges or charges of assaulting an officer --charges that Judge Steven Leifman says are usually \"avoidable felonies.\" He says the arrests often result from confrontations with police. Mentally ill people often won't do what they're told when police arrive on the scene -- confrontation seems to exacerbate their illness and they become more paranoid, delusional, and less likely to follow directions, according to Leifman. So, they end up on the ninth floor severely mentally disturbed, but not getting any real help because they're in jail. We toured the jail with Leifman. He is well known in Miami as an advocate for justice and the mentally ill. Even though we were not exactly welcomed with open arms by the guards, we were given permission to shoot videotape and tour the floor.  Go inside the 'forgotten floor' Â» . At first, it's hard to determine where the people are. The prisoners are wearing sleeveless robes. Imagine cutting holes for arms and feet in a heavy wool sleeping bag -- that's kind of what they look like. They're designed to keep the mentally ill patients from injuring themselves. That's also why they have no shoes, laces or mattresses. Leifman says about one-third of all people in Miami-Dade county jails are mentally ill. So, he says, the sheer volume is overwhelming the system, and the result is what we see on the ninth floor. Of course, it is a jail, so it's not supposed to be warm and comforting, but the lights glare, the cells are tiny and it's loud. We see two, sometimes three men -- sometimes in the robes, sometimes naked, lying or sitting in their cells. \"I am the son of the president. You need to get me out of here!\" one man shouts at me. He is absolutely serious, convinced that help is on the way -- if only he could reach the White House. Leifman tells me that these prisoner-patients will often circulate through the system, occasionally stabilizing in a mental hospital, only to return to jail to face their charges. It's brutally unjust, in his mind, and he has become a strong advocate for changing things in Miami. Over a meal later, we talk about how things got this way for mental patients. Leifman says 200 years ago people were considered \"lunatics\" and they were locked up in jails even if they had no charges against them. They were just considered unfit to be in society. Over the years, he says, there was some public outcry, and the mentally ill were moved out of jails and into hospitals. But Leifman says many of these mental hospitals were so horrible they were shut down. Where did the patients go? Nowhere. The streets. They became, in many cases, the homeless, he says. They never got treatment. Leifman says in 1955 there were more than half a million people in state mental hospitals, and today that number has been reduced 90 percent, and 40,000 to 50,000 people are in mental hospitals. The judge says he's working to change this. Starting in 2008, many inmates who would otherwise have been brought to the \"forgotten floor\"  will instead be sent to a new mental health facility -- the first step on a journey toward long-term treatment, not just punishment. Leifman says it's not the complete answer, but it's a start. Leifman says the best part is that it's a win-win solution. The patients win, the families are relieved, and the state saves money by simply not cycling these prisoners through again and again. And, for Leifman, justice is served. E-mail to a friend .<|im_end|><|im_start|>assistant<|im_sep|>Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
            "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
            "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
            "Leifman says the system is unjust and he's fighting for change .<|im_end|>\n",
            "\n",
            "Total original dataset size: 287113\n",
            "Using 1% subset size: 2871\n",
            "Train dataset size: 2583\n",
            "Validation dataset size: 288\n"
          ]
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-4\",\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "cnn_dm_dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "\n",
        "def format_cnn_dm_for_chat(examples):\n",
        "    texts = []\n",
        "    for article, summary in zip(examples['article'], examples['highlights']):\n",
        "        article_words = article.split()\n",
        "        if len(article_words) > 800:\n",
        "            article = \" \".join(article_words[:800]) + \"...\"\n",
        "\n",
        "        conversation = [\n",
        "            {\"from\": \"human\", \"value\": f\"Please summarize the following article:\\n\\n{article}\"},\n",
        "            {\"from\": \"gpt\", \"value\": summary}\n",
        "        ]\n",
        "\n",
        "        text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "total_size = len(cnn_dm_dataset['train'])\n",
        "sample_size = int(0.01 * total_size)\n",
        "subset_dataset = cnn_dm_dataset['train'].select(range(sample_size))\n",
        "\n",
        "formatted_dataset = subset_dataset.map(format_cnn_dm_for_chat, batched=True)\n",
        "\n",
        "print(\"Formatted sample:\")\n",
        "print(formatted_dataset[0]['text'])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "print(formatted_dataset[1]['text'])\n",
        "\n",
        "train_size = int(0.9 * len(formatted_dataset))\n",
        "train_dataset = formatted_dataset.select(range(train_size))\n",
        "val_dataset = formatted_dataset.select(range(train_size, len(formatted_dataset)))\n",
        "\n",
        "print(f\"\\nTotal original dataset size: {total_size}\")\n",
        "print(f\"Using 1% subset size: {sample_size}\")\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Setting Up Training Arguments and Fine-tune Model\n",
        "\n",
        "Use SFTTrainer with Unsloth to fine-tune the model."
      ],
      "metadata": {
        "id": "9jucjRvKxgtF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KF5DAHwqMFrP"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        warmup_steps = 2,\n",
        "        max_steps = 50,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = [],\n",
        "        dataloader_drop_last = True,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "from tqdm import tqdm\n",
        "\n",
        "predictions_before = []\n",
        "references = []\n",
        "\n",
        "# dÃ¹ng 10 máº«u Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ nhanh\n",
        "for sample in tqdm(cnn_dm_dataset[\"validation\"].select(range(10))):\n",
        "    article = sample[\"article\"]\n",
        "    reference_summary = sample[\"highlights\"]\n",
        "\n",
        "    prompt = f\"Summarize this article: {article}\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    pred_summary = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "    predictions_before.append(pred_summary)\n",
        "    references.append(reference_summary)\n",
        "\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "results_before = rouge.compute(\n",
        "    predictions=predictions_before,\n",
        "    references=references,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPDdplFAk1uE",
        "outputId": "b309aff0-31e6-4909-b0f1-869110b359e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:59<00:00, 17.95s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FitkamhHMFrQ",
        "outputId": "06247708-9fb3-45aa-cadd-969c89b1e709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== START TRAINING ===\n",
            "Dataset size: 2583\n",
            "Batch size: 4\n",
            "Total steps: 50\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,583 | Num Epochs = 1 | Total steps = 50\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 65,536,000/4,000,000,000 (1.64% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 30:54, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.027900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.924400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.905500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.944200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.867700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.828600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.866500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.891000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.891300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.872900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== END TRAINING ===\n",
            "Training loss: 1.9020\n",
            "Total training time: 1887.4543 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"=== START TRAINING ===\")\n",
        "print(f\"Dataset size: {len(train_dataset)}\")\n",
        "print(f\"Batch size: 4\")\n",
        "print(f\"Total steps: 50\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n=== END TRAINING ===\")\n",
        "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"Total training time: {trainer_stats.metrics.get('train_runtime', 'N/A')} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Get Some Examples output"
      ],
      "metadata": {
        "id": "FqE7DzkYxmnW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WeotQZulMFrQ",
        "outputId": "71b32b59-78d6-4d28-979a-b1d23298c716",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TESTING SUMMARIZATION MODEL ===\n",
            "\n",
            "Test 1:\n",
            "Article: Scientists have discovered a new species of deep-sea fish in the Pacific Ocean. The fish, found at d...\n",
            "Summary: Scientists discover new species of deep-sea fish in the Pacific Ocean .\n",
            "Fish found at depths of over 3,000 meters has unique bioluminescent properties .\n",
            "Researchers believe discovery could lead to new insights about marine biodiversity and evolution .\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test 2:\n",
            "Article: The stock market experienced significant volatility today as investors reacted to new economic data....\n",
            "Summary: Stock market experiences significant volatility today .\n",
            "Technology stocks lead the decline, with major companies seeing drops of up to 5% .\n",
            "Market analysts suggest the volatility is related to concerns about inflation and interest rate policies .\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test 3:\n",
            "Article: A breakthrough in renewable energy technology has been announced by researchers at MIT. The new sola...\n",
            "Summary: New solar panel design achieves 40% efficiency .\n",
            "Researchers at MIT say breakthrough could make solar energy more cost-effective .\n",
            "This could accelerate the transition to clean energy .\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell test model - cáº­p nháº­t Ä‘á»ƒ test summarization\n",
        "# Test model vá»›i má»™t vÃ i sample\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# Load má»™t vÃ i sample tá»« validation set cá»§a CNN/DailyMail Ä‘á»ƒ test\n",
        "test_articles = [\n",
        "    \"Scientists have discovered a new species of deep-sea fish in the Pacific Ocean. The fish, found at depths of over 3000 meters, has unique bioluminescent properties that help it navigate in the dark waters. Researchers believe this discovery could lead to new insights about marine biodiversity and evolution.\",\n",
        "\n",
        "    \"The stock market experienced significant volatility today as investors reacted to new economic data. Technology stocks led the decline, with major companies seeing drops of up to 5%. Market analysts suggest the volatility is related to concerns about inflation and interest rate policies.\",\n",
        "\n",
        "    \"A breakthrough in renewable energy technology has been announced by researchers at MIT. The new solar panel design can achieve 40% efficiency, significantly higher than current commercial panels. This advancement could make solar energy more cost-effective and accelerate the transition to clean energy.\"\n",
        "]\n",
        "\n",
        "print(\"=== TESTING SUMMARIZATION MODEL ===\\n\")\n",
        "\n",
        "for i, article in enumerate(test_articles, 1):\n",
        "    print(f\"Test {i}:\")\n",
        "    print(\"Article:\", article[:100] + \"...\" if len(article) > 100 else article)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"Please summarize the following article:\\n\\n{article}\"}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True,\n",
        "        return_tensors = \"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids = inputs,\n",
        "        max_new_tokens = 100,\n",
        "        use_cache = True,\n",
        "        temperature = 0.3,\n",
        "        do_sample = True,\n",
        "        pad_token_id = tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    print(\"Summary:\", response.strip())\n",
        "    print(\"-\" * 80)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluate The Model"
      ],
      "metadata": {
        "id": "d3pXp3vPxp7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_after = []\n",
        "references_after = []\n",
        "\n",
        "# dÃ¹ng 10 máº«u Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ nhanh\n",
        "for sample in tqdm(cnn_dm_dataset[\"validation\"].select(range(10))):\n",
        "    article = sample[\"article\"]\n",
        "    reference_summary = sample[\"highlights\"]\n",
        "\n",
        "    prompt = f\"Summarize this article: {article}\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    pred_summary = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "    predictions_after.append(pred_summary)\n",
        "    references_after.append(reference_summary)\n",
        "\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "results_after = rouge.compute(\n",
        "    predictions=predictions_after,\n",
        "    references=references_after,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True\n",
        ")\n",
        "print(f\"ROUGE-L F1: {results_after['rougeL']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhVwCW7KbLup",
        "outputId": "1fe98523-d61b-4ac6-f45c-eae5ca0de7c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:13<00:00,  7.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-L F1: 0.2129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ROUGE values\n",
        "rouge_names = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
        "before_scores = [results_before['rouge1'], results_before['rouge2'], results_before['rougeL']]\n",
        "after_scores  = [results_after['rouge1'], results_after['rouge2'], results_after['rougeL']]\n",
        "\n",
        "x = range(len(rouge_names))\n",
        "bar_width = 0.35\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(x, before_scores, width=bar_width, label='Before Fine-tuning', color='skyblue')\n",
        "plt.bar([i + bar_width for i in x], after_scores, width=bar_width, label='After Fine-tuning', color='salmon')\n",
        "\n",
        "plt.xlabel('ROUGE Score Type')\n",
        "plt.ylabel('Score')\n",
        "plt.title('ROUGE Score Comparison Before vs After Fine-Tuning')\n",
        "plt.xticks([i + bar_width/2 for i in x], rouge_names)\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "6qmJs0ASlmh7",
        "outputId": "89e8835f-2113-4761-8f11-6705ae0377e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'results_before' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-225a2c03cc0c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ROUGE values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrouge_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ROUGE-1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ROUGE-2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ROUGE-L'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbefore_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresults_before\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_before\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_before\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rougeL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mafter_scores\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresults_after\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_after\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_after\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rougeL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results_before' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_loss(trainer):\n",
        "    logs = trainer.state.log_history\n",
        "    steps = []\n",
        "    losses = []\n",
        "\n",
        "    for log in logs:\n",
        "        if \"loss\" in log and \"step\" in log:\n",
        "            steps.append(log[\"step\"])\n",
        "            losses.append(log[\"loss\"])\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(steps, losses, marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
        "    plt.xlabel(\"Training Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss Curve\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_training_loss(trainer)\n"
      ],
      "metadata": {
        "id": "RMB76H6McPpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Login and Save Model to HuggingFace"
      ],
      "metadata": {
        "id": "OYc9uDxgxtYY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NYGwch_MFrR"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"thanhle1702/phii4-finetuned-cnn-dailymail\")\n",
        "tokenizer.push_to_hub(\"thanhle1702/phii4-finetuned-cnn-dailymail\")"
      ],
      "metadata": {
        "id": "olvkXSjONhzX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f37294263364bdd95e50916cec83ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49659e262cf34db59bdf062e30b13d5c",
              "IPY_MODEL_3c048397fe4e43c79b1c20ef5fbf4281",
              "IPY_MODEL_350ae176c316473b98f846bb7f56e2ad"
            ],
            "layout": "IPY_MODEL_4f4f50a93e74439f9bece3cc8953a529"
          }
        },
        "49659e262cf34db59bdf062e30b13d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fc276b800f0469f83dc767312d7252d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fe51d860090c4dbb8952ddbd2bc0ca65",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "3c048397fe4e43c79b1c20ef5fbf4281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45457026c9bc4dd1b1780c7561da8baa",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28bef971c9b14b9c8e9a1a5d4bd02c64",
            "value": 3
          }
        },
        "350ae176c316473b98f846bb7f56e2ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ba9efc9d55046ec91622665d701275a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2505c01d00dd4cad81c8f78953de4a11",
            "value": "â€‡3/3â€‡[00:57&lt;00:00,â€‡16.44s/it]"
          }
        },
        "4f4f50a93e74439f9bece3cc8953a529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc276b800f0469f83dc767312d7252d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe51d860090c4dbb8952ddbd2bc0ca65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45457026c9bc4dd1b1780c7561da8baa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28bef971c9b14b9c8e9a1a5d4bd02c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ba9efc9d55046ec91622665d701275a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2505c01d00dd4cad81c8f78953de4a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
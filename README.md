# NLP Assignment (CO3086) - Transformer Models and Fine-Tuning
## Team Members


## Overview
This repository implements experiments with LLaMA 3.1-8B, Phi-4, and LLaMA 3-8B, applying three fine-tuning techniques (LoRA, Adapter, Prompt Tuning) on three NLP tasks.

## Models
- LLaMA 3.1-8B: Fine-tuned with LoRA.
- Phi-4: 
- LLaMA 3-8B: 

## Setup
1. Clone: `git clone <repo_url>`
2. Install: `pip install -r requirements.txt`
3. Run: `bash scripts/setup_environment.sh`

## Experiments
- Tasks: 
- Run: `bash scripts/run_all_experiments.sh`
- Results: `bash scripts/generate_results.sh`

## Report
The report is located in `docs/report/report.pdf`

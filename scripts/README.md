# BTL_NLP Training Scripts

CÃ¡c script training nÃ y thay tháº¿ hoÃ n toÃ n cÃ¡c Jupyter notebook, sá»­ dá»¥ng framework modular má»›i vá»›i cÃ¡c tÃ­nh nÄƒng Ä‘Æ°á»£c cáº£i thiá»‡n.

## ğŸ“‹ Danh sÃ¡ch Scripts

### Llama3 8B Experiments
- `train_llama3_cnn_dailymail.py` - Fine-tune Llama3-8B trÃªn CNN/DailyMail (summarization)
- `train_llama3_wmt14.py` - Fine-tune Llama3-8B trÃªn WMT14 (translation) 
- `train_llama3_sst2.py` - Fine-tune Llama3-8B trÃªn SST2 (classification)

### Phi-4 Mini Experiments  
- `train_phi4_cnn_dailymail.py` - Fine-tune Phi-4 Mini trÃªn CNN/DailyMail (summarization)
- `train_phi4_wmt14.py` - Fine-tune Phi-4 Mini trÃªn WMT14 (translation)
- `train_phi4_sst2.py` - Fine-tune Phi-4 Mini trÃªn SST2 (classification)

### Llama3 1B Experiments
- `train_llama3_1b_cnn_dailymail.py` - Fine-tune Llama3-1B trÃªn CNN/DailyMail (summarization)
- `train_llama3_1b_wmt14.py` - Fine-tune Llama3-1B trÃªn WMT14 (translation)
- `train_llama3_1b_sst2.py` - Fine-tune Llama3-1B trÃªn SST2 (classification)

### Master Script
- `run_all_experiments.py` - Script chÃ­nh Ä‘á»ƒ cháº¡y táº¥t cáº£ hoáº·c má»™t sá»‘ experiments

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Cháº¡y má»™t script Ä‘Æ¡n láº»

```bash
cd scripts

# Cháº¡y Llama3 trÃªn CNN/DailyMail
python train_llama3_cnn_dailymail.py

# Cháº¡y Phi-4 trÃªn WMT14
python train_phi4_wmt14.py

# Cháº¡y Llama3-1B trÃªn SST2  
python train_llama3_1b_sst2.py
```

### 2. Sá»­ dá»¥ng Master Script

```bash
cd scripts

# Xem danh sÃ¡ch táº¥t cáº£ experiments
python run_all_experiments.py --list

# Cháº¡y táº¥t cáº£ experiments
python run_all_experiments.py --all

# Cháº¡y experiments cá»¥ thá»ƒ
python run_all_experiments.py llama3_cnn phi4_wmt14

# Cháº¡y táº¥t cáº£ experiments cá»§a Llama3
python run_all_experiments.py --model llama3

# Cháº¡y táº¥t cáº£ experiments classification
python run_all_experiments.py --task classification

# Cháº¡y táº¥t cáº£ experiments translation  
python run_all_experiments.py --task translation

# Cháº¡y táº¥t cáº£ experiments summarization
python run_all_experiments.py --task summarization
```

## âš™ï¸ Cáº¥u hÃ¬nh

CÃ¡c scripts sá»­ dá»¥ng cáº¥u hÃ¬nh Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho tá»«ng task:

### Summarization (CNN/DailyMail)
- **Learning rate**: 1e-4 (tháº¥p hÆ¡n cho summarization)
- **Epochs**: 2
- **LoRA rank**: 16
- **Sequence length**: 2048

### Translation (WMT14)  
- **Learning rate**: 2e-4 (cao hÆ¡n cho translation)
- **Epochs**: 3
- **LoRA rank**: 32 (cao hÆ¡n cho translation)
- **Warmup steps**: 100

### Classification (SST2)
- **Learning rate**: 3e-4 (cao nháº¥t cho classification)
- **Epochs**: 5 (nhiá»u hÆ¡n cho classification)
- **LoRA rank**: 8 (tháº¥p nháº¥t cho classification)
- **Sequence length**: 512 (ngáº¯n hÆ¡n)

## ğŸ’¾ Káº¿t quáº£

Táº¥t cáº£ models Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `./results/`:

```
results/
â”œâ”€â”€ llama3_8b_cnn_dailymail/      # Llama3-8B CNN/DailyMail results
â”œâ”€â”€ llama3_8b_wmt14/              # Llama3-8B WMT14 results  
â”œâ”€â”€ llama3_8b_sst2/               # Llama3-8B SST2 results
â”œâ”€â”€ phi4_mini_cnn_dailymail/      # Phi-4 CNN/DailyMail results
â”œâ”€â”€ phi4_mini_wmt14/              # Phi-4 WMT14 results
â”œâ”€â”€ phi4_mini_sst2/               # Phi-4 SST2 results
â”œâ”€â”€ llama3_1b_cnn_dailymail/      # Llama3-1B CNN/DailyMail results
â”œâ”€â”€ llama3_1b_wmt14/              # Llama3-1B WMT14 results
â””â”€â”€ llama3_1b_sst2/               # Llama3-1B SST2 results
```

Má»—i thÆ° má»¥c chá»©a:
- **model files** (pytorch_model.bin, config.json, etc.)
- **tokenizer files** 
- **training logs**
- **evaluation results**

## ğŸ“Š YÃªu cáº§u há»‡ thá»‘ng

| Model | Task | VRAM Required | Estimated Time |
|-------|------|---------------|----------------|
| Llama3-8B | Summarization | 16GB | 2-3 hours |
| Llama3-8B | Translation | 16GB | 3-4 hours |
| Llama3-8B | Classification | 12GB | 1-2 hours |
| Phi-4 Mini | Summarization | 12GB | 1.5-2.5 hours |
| Phi-4 Mini | Translation | 12GB | 2-3 hours |
| Phi-4 Mini | Classification | 8GB | 1-1.5 hours |
| Llama3-1B | Summarization | 8GB | 1-1.5 hours |
| Llama3-1B | Translation | 8GB | 1-2 hours |
| Llama3-1B | Classification | 6GB | 0.5-1 hour |

## ğŸ”§ TÃ¹y chá»‰nh

Äá»ƒ tÃ¹y chá»‰nh cáº¥u hÃ¬nh training, báº¡n cÃ³ thá»ƒ:

1. **Chá»‰nh sá»­a trá»±c tiáº¿p trong script**:
```python
config = TrainingConfig(
    num_train_epochs=5,        # TÄƒng sá»‘ epochs
    learning_rate=1e-4,        # Thay Ä‘á»•i learning rate
    per_device_train_batch_size=4,  # TÄƒng batch size
    max_samples=10000          # TÄƒng sá»‘ samples
)
```

2. **Sá»­ dá»¥ng config file** (xem `config.yaml` á»Ÿ root)

3. **Thay Ä‘á»•i model parameters**:
```python
model, tokenizer = load_llama3_model(
    lora_r=32,        # TÄƒng LoRA rank
    use_dora=True,    # Sá»­ dá»¥ng DoRA
    max_seq_length=4096  # TÄƒng sequence length
)
```

## ğŸ› Troubleshooting

### Out of Memory (OOM)
- Giáº£m `per_device_train_batch_size`
- TÄƒng `gradient_accumulation_steps`
- Giáº£m `max_seq_length`
- Sá»­ dá»¥ng `fp16=True` thay vÃ¬ `bf16=True`

### Slow Training
- TÄƒng `per_device_train_batch_size`
- Giáº£m `gradient_accumulation_steps`
- Giáº£m `max_samples` Ä‘á»ƒ test nhanh

### Dependencies Issues
- Cháº¡y `pip install -r requirements.txt`
- Äáº£m báº£o CUDA Ä‘Æ°á»£c cÃ i Ä‘áº·t Ä‘Ãºng
- Kiá»ƒm tra PyTorch version compatibility

## ğŸ“ˆ Monitoring

Scripts tá»± Ä‘á»™ng hiá»ƒn thá»‹:
- **Environment info** (GPU, memory, CUDA version)
- **Model parameters** (trainable params, total params)
- **Training progress** (loss, learning rate)
- **Evaluation results** (task-specific metrics)
- **Timing information** (estimated completion time)

## ğŸ¯ So sÃ¡nh vá»›i Notebooks

| Feature | Notebooks | Scripts |
|---------|-----------|---------|
| Code duplication | 100+ lines repeated | 3-5 lines |
| Configuration | Hard-coded | Centralized config |
| Error handling | Manual | Automatic |
| Logging | Minimal | Comprehensive |
| Reproducibility | Variable | Consistent |
| Memory optimization | Manual | Automatic |
| Model comparison | Difficult | Easy |

Scripts mang láº¡i hiá»‡u quáº£ vÃ  tÃ­nh nháº¥t quÃ¡n cao hÆ¡n nhiá»u so vá»›i viá»‡c sá»­ dá»¥ng notebooks riÃªng láº»! 